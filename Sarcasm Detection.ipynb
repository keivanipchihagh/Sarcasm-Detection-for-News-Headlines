{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boolean-creek",
   "metadata": {},
   "source": [
    "# Sarcasm Detection for News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-evans",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greatest-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-university",
   "metadata": {},
   "source": [
    "## Read News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enabling-contractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample: {'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n"
     ]
    }
   ],
   "source": [
    "def parse_data(file):\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l)\n",
    "\n",
    "data = list(parse_data('datasets/Sarcasm_Headlines_Dataset.json'))\n",
    "\n",
    "print('Data Sample:', data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-chess",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fewer-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "vocab_size = 10000\n",
    "max_len = 100\n",
    "padding_type = 'post'\n",
    "truncating_type = 'post'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-aircraft",
   "metadata": {},
   "source": [
    "### Dividing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "czech-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "labels = []\n",
    "\n",
    "for news in data:\n",
    "    headlines.append(news['headline'])\n",
    "    labels.append(news['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-effects",
   "metadata": {},
   "source": [
    "### Spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "domestic-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = headlines[:training_size]\n",
    "training_labels = labels[:training_size]\n",
    "\n",
    "testing_data = headlines[training_size:]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-silly",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "muslim-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[328, 1, 799, 3405, 2404, 47, 389, 2214, 1, 6, 2614, 8863], [4, 6840, 3096, 3097, 23, 2, 161, 1, 390, 2842, 6, 251, 9, 889], [153, 890, 2, 891, 1445, 2215, 595, 5650, 221, 133, 36, 45, 2, 8864]]\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer object\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = '<OOV>')\n",
    "\n",
    "# Create a word_index dictionary\n",
    "tokenizer.fit_on_texts(training_data)\n",
    "\n",
    "# Get word_index dictionary\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Tokenize and pad training data\n",
    "training_sequences = tokenizer.texts_to_sequences(training_data)\n",
    "training_data_padded = pad_sequences(training_sequences, maxlen = max_len, padding = padding_type, truncating = truncating_type)\n",
    "\n",
    "# Tokenize and pad testing data\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_data)\n",
    "testing_data_padded = pad_sequences(testing_sequences, maxlen = max_len, padding = padding_type, truncating = truncating_type)\n",
    "\n",
    "# Sequence sample\n",
    "print(training_sequences[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-banner",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-cuisine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-wyoming",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
